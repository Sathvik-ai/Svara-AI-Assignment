{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reply Classification ML Pipeline\n",
    "\n",
    "This notebook implements a complete ML pipeline for classifying email replies into positive, negative, and neutral categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2129, 2)\n",
      "Columns: ['reply', 'label']\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "neutral     704\n",
      "positive    446\n",
      "NEGATIVE    267\n",
      "POSITIVE    263\n",
      "Negative    254\n",
      "negative    189\n",
      "Neutral       3\n",
      "NEUTRAL       2\n",
      "Positive      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('reply_classification_dataset.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: (321, 3)\n",
      "Label distribution: label\n",
      "neutral     121\n",
      "positive    114\n",
      "negative     86\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['label'] = df['label'].str.lower()\n",
    "df_clean = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[?!]{2,}', '', text)\n",
    "    text = re.sub(r',+', ',', text)\n",
    "    text = text.replace(' u ', ' you ')\n",
    "    text = text.replace(' plz ', ' please ')\n",
    "    text = text.replace(' w/ ', ' with ')\n",
    "    text = text.replace('schdule', 'schedule')\n",
    "    text = text.replace('intrsted', 'interested')\n",
    "    text = text.replace('alredy', 'already')\n",
    "    text = text.replace('oppurtunity', 'opportunity')\n",
    "    text = text.replace('intrest', 'interest')\n",
    "    text = text.replace('commited', 'committed')\n",
    "    text = text.replace('lets', 'let us')\n",
    "    return text\n",
    "\n",
    "df_clean['cleaned_text'] = df_clean['reply'].apply(preprocess_text)\n",
    "print(f\"After cleaning: {df_clean.shape}\")\n",
    "print(f\"Label distribution: {df_clean['label'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 256 samples\n",
      "Test set: 65 samples\n",
      "TF-IDF features: 458\n"
     ]
    }
   ],
   "source": [
    "X = df_clean['cleaned_text']\n",
    "y = df_clean['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Accuracy: 0.9538\n",
      "F1 Score: 0.9544\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.94      0.97        17\n",
      "     neutral       1.00      0.92      0.96        25\n",
      "    positive       0.88      1.00      0.94        23\n",
      "\n",
      "    accuracy                           0.95        65\n",
      "   macro avg       0.96      0.95      0.96        65\n",
      "weighted avg       0.96      0.95      0.95        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_pred = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, lr_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, lr_pred, average='weighted'):.4f}\")\n",
    "print(f\"\\n{classification_report(y_test, lr_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "Accuracy: 0.8923\n",
      "F1 Score: 0.8929\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      1.00      0.85        17\n",
      "     neutral       1.00      0.76      0.86        25\n",
      "    positive       0.96      0.96      0.96        23\n",
      "\n",
      "    accuracy                           0.89        65\n",
      "   macro avg       0.90      0.91      0.89        65\n",
      "weighted avg       0.92      0.89      0.89        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "rf_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, rf_pred, average='weighted'):.4f}\")\n",
    "print(f\"\\n{classification_report(y_test, rf_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Results:\n",
      "Accuracy: 0.9538\n",
      "F1 Score: 0.9544\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.94      0.97        17\n",
      "     neutral       1.00      0.92      0.96        25\n",
      "    positive       0.88      1.00      0.94        23\n",
      "\n",
      "    accuracy                           0.95        65\n",
      "   macro avg       0.96      0.95      0.96        65\n",
      "weighted avg       0.96      0.95      0.95        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(probability=True, random_state=42)\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', lr_model),\n",
    "        ('svm', svm_model),\n",
    "        ('nb', nb_model)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble_model.fit(X_train_tfidf, y_train)\n",
    "ensemble_pred = ensemble_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Ensemble Model Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, ensemble_pred):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, ensemble_pred, average='weighted'):.4f}\")\n",
    "print(f\"\\n{classification_report(y_test, ensemble_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "best_model = ensemble_model\n",
    "\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "print(\"Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'I'm excited to see the demo!'\n",
      "Prediction: positive (Confidence: 0.763)\n",
      "\n",
      "Text: 'Not interested, please remove me'\n",
      "Prediction: negative (Confidence: 0.728)\n",
      "\n",
      "Text: 'Can you send pricing details?'\n",
      "Prediction: neutral (Confidence: 0.821)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_reply(text, model, vectorizer):\n",
    "    processed_text = preprocess_text(text)\n",
    "    text_tfidf = vectorizer.transform([processed_text])\n",
    "    prediction = model.predict(text_tfidf)[0]\n",
    "    probabilities = model.predict_proba(text_tfidf)[0]\n",
    "    confidence = max(probabilities)\n",
    "    return prediction, confidence\n",
    "\n",
    "test_texts = [\n",
    "    \"I'm excited to see the demo!\",\n",
    "    \"Not interested, please remove me\",\n",
    "    \"Can you send pricing details?\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    pred, conf = predict_reply(text, best_model, vectorizer)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Prediction: {pred} (Confidence: {conf:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svara_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
